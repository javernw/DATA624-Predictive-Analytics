---
title: "DATA624 Homework 5"
author: "Javern Wilson"
date: "3/1/2020"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
    theme: flatly
    highlight: pygments
---

```{r message=FALSE, warning=FALSE}
library(fpp2)
library(tidyverse)
```


## 7.1

Consider the `pigs` series — the number of pigs slaughtered in Victoria each month. 

a. Use the `ses()` function in R to find the optimal values of $\alpha$ and $\ell_0$, and generate forecasts for the next four months.

```{r}
pigs_ses <- ses(pigs, h=4)
pigs_ses[["model"]]
pigs_ses
```

$\alpha = 0.2971$ and $\ell_0 = 77260.0561$

b. Compute a 95% prediction interval for the first forecast using $\hat{y} \pm 1.96s$ where $s$ is the standard deviation of the residuals. Compare your interval with the interval produced by R.

```{r}
s <- sd(pigs_ses$residuals)
pigs_ses$mean[1]-(1.96*s) #lower
pigs_ses$mean[1]+(1.96*s) #upper
```


```{r}
ses(pigs, level = 95, h = 4)$lower[1]
ses(pigs, level = 95, h = 4)$upper[1]

```

The intervals are close but not identical.



## 7.5

Data set `books` contains the daily sales of paperback and hardcover books at the same store. The task is to forecast the next four days’ sales for paperback and hardcover books.

a. Plot the series and discuss the main features of the data.

```{r}
autoplot(books) +  
  ylab("Books") + xlab("Time") + 
  ggtitle("Daily Sales of Paperback and Hardcover Books")
```

Overall there are more sales in Hardcover books than do Paperback books. However, both types of books have a pattern resembling a positive trend. The peaks and troughs happens at irregular intervals therefore there is not any reason to conclude there is seasonality present.

b. Use the ses() function to forecast each series, and plot the forecasts.

```{r}
paperback <- ses(books[,"Paperback"], h=4)
hardcover <- ses(books[, "Hardcover"], h=4)

autoplot(books) +
  autolayer(paperback) +
  autolayer(hardcover) +
  ylab("Books)") + xlab("Time")
```


c. Compute the RMSE values for the training data in each case.

```{r}
round(accuracy(paperback), 2)
round(accuracy(hardcover), 2)
```


## 7.6

We will continue with the daily sales of paperback and hardcover books in data set `books`.

a. Apply Holt’s linear method to the paperback and hardback series and compute four-day forecasts in each case.

```{r}
paperback2 <- holt(books[,"Paperback"], h=4)
hardcover2 <- holt(books[, "Hardcover"], h=4)

paperback2
hardcover2

autoplot(books) +
  autolayer(paperback2) +
  autolayer(hardcover2) +
  ylab("Books)") + xlab("Time")
```


b. Compare the RMSE measures of Holt’s method for the two series to those of simple exponential smoothing in the previous question. (Remember that Holt’s method is using one more parameter than SES.) Discuss the merits of the two forecasting methods for these data sets.

```{r}
round(accuracy(paperback2), 2)
round(accuracy(hardcover2), 2)
```

The RMSE scores are lower with the holt method for both paperback and hardcover books. This is an obvious improvement.

c. Compare the forecasts for the two series using both methods. Which do you think is best?

The RMSE scores are lower with the holt method so this method is better.


d. Calculate a 95% prediction interval for the first forecast for each series, using the RMSE values and assuming normal errors. Compare your intervals with those produced using ses and holt.

Paperback
```{r}
s_p <- round(accuracy(paperback2), 2)[2] 

cat( "RMSE:",
paperback2$mean[1]-(1.96*s_p), 
paperback2$mean[1]+(1.96*s_p)) 

cat("\nSES:",
  ses(books[, "Paperback"], level = 95, h = 4)$lower[1],
  ses(books[, "Paperback"], level = 95, h = 4)$upper[1])

cat("\nHOLT:",
    holt(books[, "Paperback"], level = 95, h = 4)$lower[1],
    holt(books[, "Paperback"], level = 95, h = 4)$upper[1])
```


Hardcover
```{r}
s_p <- round(accuracy(paperback2), 2)[2] 

cat( "RMSE:",
hardcover2$mean[1]-(1.96*s_p), 
hardcover2$mean[1]+(1.96*s_p)) 

cat("\nSES:",
  ses(books[, "Hardcover"], level = 95, h = 4)$lower[1],
  ses(books[, "Hardcover"], level = 95, h = 4)$upper[1])

cat("\nHOLT:",
    holt(books[, "Hardcover"], level = 95, h = 4)$lower[1],
    holt(books[, "Hardcover"], level = 95, h = 4)$upper[1])
```

The intervals I produced are closer to the ones derived from the holt method.


## 7.7

For this exercise use data set `eggs`, the price of a dozen eggs in the United States from 1900–1993. Experiment with the various options in the `holt()` function to see how much the forecasts change with damped trend, or with a Box-Cox transformation. Try to develop an intuition of what each argument is doing to the forecasts.

[Hint: use `h=100` when calling holt() so you can clearly see the differences between the various options when plotting the forecasts.]

Damp trend
```{r}
e1 <- holt(eggs, h=100)
e2 <- holt(eggs, damped=TRUE, h=100)
e3 <- holt(eggs, lambda = "auto", h=100) #-- lambda= 0.3956
autoplot(eggs) +
  autolayer(e1, series="Holt's method", PI=FALSE) +
  autolayer(e2, series="Damped Holt's method", PI=FALSE) +
  autolayer(e3, series="Box-Cox", PI=FALSE)+
  ggtitle("Forecasts from Holt's method") + xlab("Year") +
  ylab("Eggs") +
  guides(colour=guide_legend(title="Forecast"))

```

Which model gives the best RMSE?

```{r}
e1 %>% accuracy() # holt linear method
e2 %>% accuracy() # holt damp method
e3 %>% accuracy() # holt with box-cox transformation 
```

Based on the accuracy scores provided, the model with box-cox transformation parameter is best.



## 7.8 

Recall your retail time series data (from Exercise 3 in Section 2.10).

```{r}
retaildata <- readxl::read_excel("retail.xlsx", skip=1)
myts <- ts(retaildata[,"A3349399C"], frequency=12, start=c(1982,4)) #clothing

autoplot(myts)
```


a. Why is multiplicative seasonality necessary for this series?

The multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series which is happening in this case.

b. Apply Holt-Winters’ multiplicative method to the data. Experiment with making the trend damped.

```{r, fig.width= 10, fig.height= 9}
fit1 <- hw(myts, seasonal="multiplicative")
fit2 <- hw(myts, damped = T, seasonal="multiplicative")
autoplot(myts) +
  autolayer(fit1, series="HW multiplicative forecasts", PI=FALSE) +
  autolayer(fit2, series="HW multi damped",
    PI=FALSE) +
  xlab("Year") +
  ylab("Clothing") +
  ggtitle("Retail: Turnover") +
  guides(colour=guide_legend(title="Forecast"))
```


c. Compare the RMSE of the one-step forecasts from the two methods. Which do you prefer?

```{r}
fit1 %>% accuracy() # multiplicative
fit2 %>% accuracy() # damped multiplicative
```

The damped multiplicative method is better due to the lower RMSE score.

d. Check that the residuals from the best method look like white noise.

```{r}
ggAcf(fit2$residuals)
Box.test(fit2$residuals, lag = 24, fitdf = 0, type = "Ljung")
```

The p-value is less than 0.05 therefore this suggests that that the data is not from white noise.

e. Now find the test set RMSE, while training the model to the end of 2010. Can you beat the seasonal naïve approach from Exercise 8 in Section 3.7?

```{r}
myts.train <- window(myts, end=c(2010,12))
myts.test <- window(myts, start=2011)

fc1 <- hw(myts.train, seasonal="multiplicative")
accuracy(fc1, myts.test)

fc2 <- snaive(myts.train)
accuracy(fc2, myts.test)
```

Yes, the holt method does way better than the seasonal naive method.


## 7.9

For the same retail data, try an STL decomposition applied to the Box-Cox transformed series, followed by ETS on the seasonally adjusted data. How does that compare with your best previous forecasts on the test set?

#### STL
```{r}
lambda <- BoxCox.lambda(myts.train)
stl_bc_myts.train <- stlf(myts.train, lambda = lambda)

autoplot(myts.train, series = "train") +
  autolayer(stl_bc_myts.train, series = 'STL')

accuracy(stl_bc_myts.train, myts.test)
```

#### ETS
```{r}

ets_myts.train <- ets(seasadj(decompose(myts.train,"multiplicative")))
summary(ets_myts.train)
autoplot(myts.train, series = 'Train') + 
  autolayer(forecast(ets_myts.train, h = 24, PI=F), series = "Forecast")
accuracy(forecast(ets_myts.train), myts.test)
```

The previous forecasts (STL) are better due to their RMSE score being lower than the one produced by ETS.
